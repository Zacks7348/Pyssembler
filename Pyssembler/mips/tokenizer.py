from enum import Enum
from typing import Union
import re
import string

from .hardware import registers
from .hardware.types import DataType
from .instructions import instruction_set as instr_set
from .utils import Integer
from .directives import Directives
from Pyssembler.mips import utils
from .errors import TokenizationError
from Pyssembler.mips import directives

__SPECIAL_CHARS__ = {'"': '\"', '\\': '\\', 'n': '\n',
                     'r': '\r', 't': '\t', 'b': '\b',
                     '0': '\0'}
__DELIMITERS__ = [' ', '\t', ',']

__SPECIAL_DELIMITERS__ = ['(', ')', ':']

__VALID_SYMBOL_CHARS__ = string.ascii_letters + string.digits + '_.$'


class TokenType(Enum):
    MNEMONIC = 0
    REGISTER = 1
    IMMEDIATE = 2
    DIRECTIVE = 3
    COMMENT = 4
    LABEL = 5
    LEFT_P = 6
    RIGHT_P = 7
    ASCII = 8
    COLON = 9




class Token:
    """
    Represents a token generated by this file's tokenize function

    Attributes
    ----------
    value : str
        The value of the token
    type : TokenType
        The type of token
    linenum : int
        The line number this token was found in
    charnum : int
        The character position the token first appears at
    """

    def __init__(self, value: Union[int, str], type_: TokenType,
                 linenum=0, charnum=0) -> None:
        """
        Parameters
        ----------
        value : str
            The value of the token
        type : TokenType
            The type of token
        linenum : int, optional
            The line number this token was found in (default=0)
        charnum : int, optional
            The character position the token first appears at 
        """
        self.value = value
        self.type = type_
        self.linenum = linenum
        self.charnum = charnum

    def __str__(self):
        return 'Token(value={}, type={})'.format(repr(self.value), self.type)

    def __repr__(self):
        return 'Token(value={}, type={}, linenum={}, charnum={})'.format(
            repr(self.value), self.type, self.linenum, self.charnum
        )


def __is_valid_symbol(token_str: str) -> bool:
    """Returns true if the token_str is a valid symbol

    Symbols can contain alphanumeric characters as well as _ . $ and cannot
    start with a number
    """
    if not token_str:
        return False

    if not token_str[0].isalpha() and not token_str[0] in '_.$':
        return False
    for c in token_str[1:]:
        # Already tested first char
        if not c in __VALID_SYMBOL_CHARS__:
            return False
    return True


def __match_token(token_str: str) -> tuple:
    """
    Tries to match a token to a type. If a match is found, a tuple
    is returned that contains the type and a parsed value (if needed). The
    tuple follows the format (type, parsed value).
    In some cases, like an IMMEDIATE, the value stored in the token object
    should already be parsed to avoid repeating ourselves. Also, the original
    line and location will be stored

    Returns
    -------
    tuple
        If a match is found, return a tuple (type, parsed value)
    """
    #print('\tMatching {}'.format(repr(token_str)))
    # If token_str ends with a newline char we can safely remove it
    if token_str.endswith('\n'):
        token_str = token_str[:-1]

    #import pdb; pdb.set_trace()

    if token_str == '(':
        return TokenType.LEFT_P, token_str
    elif token_str == ')':
        return TokenType.RIGHT_P, token_str
    elif token_str == ':':
        return TokenType.COLON, token_str
    elif token_str.startswith('#'):
        return TokenType.COMMENT, token_str
    elif registers.is_register(token_str):
        return TokenType.REGISTER, registers.get_addr(token_str)
    elif Directives.is_directive(token_str):
        return TokenType.DIRECTIVE, token_str
    elif not (imm := Integer.from_string(token_str)) is None:
        # token_str is an immediate
        return TokenType.IMMEDIATE, imm
    elif token_str.startswith('"') and token_str.endswith('"') and token_str != '"':
        # token_str is an ascii string and has already been parsed
        # by the tokenizer, just need to remove surrounding double quotes
        return TokenType.ASCII, token_str[1:-1]
    elif instr_set.is_instruction(token_str):
        # token_str is an instruction mnemonic
        return TokenType.MNEMONIC, token_str
    elif __is_valid_symbol(token_str):
        return TokenType.LABEL, token_str
    else:
        return None, None


def __add_token(token_list, token_str, src_file, linenum, token_start):
    """
    If token_str is a valid token, add it to token_list
    """

    #print('\tFound potential token {}'.format(repr(token_str)))
    if token_str == '' or token_str == '\n':
        return  # token_str is empty string or just a newline char
    token_type, val = __match_token(token_str)
    if not token_type:
        # Invalid token
        raise TokenizationError(
            filename=src_file,
            linenum=linenum,
            charnum=token_start,
            message='Invalid token encountered'
        )
    token_list.append(Token(val, token_type, linenum, token_start))


def tokenize_line(line: str, src_file: str, linenum: int) -> list:
    """
    Tokenizes a line 

    Tokens are seperated by chars in __DELIMITERS__ or any chars that
    designate the start of a new token. Trailing newline characters are discarded 
    and any characters surrounded by double quotes or single quotes are tested for
    escapes

    Along with the source file and line number, the first character position is also 
    stored in the generated Tokens to allow for better error messages

    Parameters
    ----------
    line : str
        The string to tokenize
    src_file : str
        The filename the string came from
    linenum : int
        The line number of this string

    Returns
    -------
    list
        A list of :class:`Token` found in line

    Raises
    ------
    TokenizationError
        If an invalid/unknown token is found
    """

    if line.endswith('\n'):
        # Disregard trailing newline char
        line = line[:-1]

    token_list = []
    if line == '':
        return token_list

    in_ascii = False
    current = ''
    start = 1  # char number current token starts at
    prev_c = None

    i = 0
    while i < len(line):
        c = line[i]
        # Iterate over every char in line and keep track of the char number
        # (start counting at 1)
        if in_ascii:
            # Inside an ascii string (surrounded by double quotes)
            # Exit string when an unescaped double quote is read

            if c in __SPECIAL_CHARS__ and prev_c == '\\':
                # c is a special character but will be read as two chars:
                # backslash and char. Need to manually convert it
                # backslash is already in current, remove it and instead of
                # adding c, add special char
                current = current[:-1] + __SPECIAL_CHARS__[c]
            elif c == '"':
                # An escaped double quote will already have been checked for
                # in the if statement above
                __add_token(token_list, current+c, src_file, linenum, start)
                current = ''
                start = i + 1
                in_ascii = False
            else:
                current += c
        else:
            # Not in an ascii string
            if c == '#':
                # The rest of this line is a comment
                __add_token(token_list, line[i:],
                            src_file, linenum, start)
                break
            elif c in __DELIMITERS__:
                # end of token reached
                __add_token(token_list, current, src_file, linenum, start)
                current = ''
                start = i + 1
            elif c in __SPECIAL_DELIMITERS__:
                # Special delimiters signal end of token and should be
                # added as a token as well
                __add_token(token_list, current, src_file, linenum, start)
                __add_token(token_list, c, src_file, linenum, i+1)
                current = ''
                start = i + 1
            elif c == '"':
                # This is the start of a new ascii string
                __add_token(token_list, current, src_file, linenum, start)
                current = c  # Start this token with double quote
                start = i + 1
                in_ascii = True
            elif c == "'":
                # This is the start of a new character literal
                # Since characters can only be 3 or 4 chars long when read,
                # 'c' or '\n'
                # can attempt to tokenize character literal now
                __add_token(token_list, current, src_file, linenum, start)
                current = c
                start = i + 1
                if start < len(line) - 1:
                    # There is room for the character literal
                    future_c1 = line[start]
                    future_c2 = line[start+1]
                    if future_c2 == "'":
                        # End of character literal
                        current += future_c1 + future_c2
                        __add_token(token_list, current,
                                    src_file, linenum, start)
                        current = ''
                        i += 3
                        start = i + 1
                        continue
                    elif future_c1 == '\\' and future_c2 in __SPECIAL_CHARS__ and \
                            start+2 < len(line)-1:
                        # Character literal has a special character in it and
                        # line has enough space to hold another char
                        # (still need to find last ')
                        if (future_c3 := line[start+2]) == "'":
                            # End of character literal
                            current += future_c1 + future_c2 + future_c3
                            __add_token(token_list, current,
                                        src_file, linenum, start)
                            current = ''
                            i += 4
                            start = i + 1
                            continue
                # If this point has been reached, could not tokenize character
                # literal, throw a tokenization error
                raise TokenizationError(
                    filename=src_file,
                    linenum=linenum,
                    charnum=start,
                    message='Invalid token encountered')
            elif c == '\'':
                # This is the start of a character
                __add_token(token_list, current, src_file, linenum, start)
                current = c  # Start this token with single quote
                start = i + 1
            else:
                current += c
        prev_c = c
        i += 1
    if current:
        # Last token in line hasn't been processed yet
        __add_token(token_list, current, src_file, linenum, start)

    return token_list


def __add_program_line(program, source, tokens, label):
    """
    Generate a ProgramLine object and add it to the program
    """

    if tokens[-1].type == TokenType.COMMENT:
        # Ignore all comment tokens
        tokens = tokens[:-1]
    
    if len(tokens) == 0:
        # No tokens = no program line
        return

    program.create_program_line_from_source(source, tokens, label)


def tokenize_program(program):
    """
    Tokenize a mips program

    MIPS is line-oriented meaning each line is complete. Each source
    line in the program is tokenized based on the types defined in 
    :class:`TokenType`. 
    
    Once all tokens are generated for a line, a :class:`ProgramLine` 
    object is created and added to the program. Any comments that are 
    found are ignored and any invalid tokens will raise an error.

    Parameters
    ----------
    program : MIPSProgram
        the program to be tokenized

    Raises
    ------
    TokenizationError
        If an invalid/unknown token is found
    """
    label = None

    for src_line in program.src_lines:
        token_list = tokenize_line(src_line.line, src_line.src_file,
                                   src_line.linenum)

        if len(token_list) == 0:
            # No tokens found on this line
            continue
        if token_list[0].type == TokenType.LABEL:
            # First token of this line is a label, need to
            # check to see if the line this label designates is
            # this src_line or a future one. If the rest of the line
            # is not here, expect next lines to be whitespaces or the line
            if len(token_list) == 1:
                # Expected a colon after label
                raise TokenizationError(
                    filename=src_line.src_file,
                    linenum=src_line.linenum,
                    charnum=token_list[0].charnum,
                    message='Expected colon after label')
            if token_list[1].type == TokenType.COLON:
                # Valid label definition
                label = token_list[0]
                if len(token_list) >= 3:
                    # Rest of line is here
                    token_list = token_list[2:]
                    __add_program_line(program, src_line, token_list,
                                       label)
            else:
                # Expected a colon after label
                raise TokenizationError(
                    filename=src_line.src_file,
                    linenum=src_line.linenum,
                    charnum=token_list[0].charnum,
                    message='Expected colon after label')
        elif token_list[0].type == TokenType.COMMENT:
            # Don't add only-comment lines to program
            continue
        else:
            __add_program_line(program, src_line, token_list, label)
            if not label is None:
                label = None


def tokenize_instr_format(instr_format: str) -> list:
    """Special function for tokenizing an instruction format string

    Parameters
    ----------
    instr_format : :class:`str`

    Returns
    -------
    list
        A list of TokenTypes
    """

    split_format = []

    #import pdb; pdb.set_trace()

    # when iterating over instr_format.split(), start at index 1
    # since mnemonic will always be at 0

    for value in instr_format.split()[1:]:
        # Remove any trailing commas and expand tokens (if needed)
        if value.endswith(','): value = value[:-1]
        # Might need to expand some tokens
        if '(' in value and value.endswith(')'):
            # x(y)
            x, y = value.split('(')
            split_format += [x, '(', y[:-1], ')']
        else: split_format.append(value)

    token_list = []
    for value in split_format:
        # Split format string by spaces
        if value == '(':
            token_list.append(Token(value, TokenType.LEFT_P))
        elif value == ')':
            token_list.append(Token(value, TokenType.RIGHT_P))
        elif value in ('rd', 'rs', 'rt'):
            token_list.append(Token(value, TokenType.REGISTER))
        elif value == 'immediate':
            token_list.append(Token(value, TokenType.IMMEDIATE))
        else:
            # There are many names for the immediates in formats
            # offset, bp, sa, target. 
            token_list.append(Token(value, TokenType.IMMEDIATE))
    return token_list
